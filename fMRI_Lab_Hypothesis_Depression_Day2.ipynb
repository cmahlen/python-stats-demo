{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eb445ca903",
   "metadata": {},
   "source": [
    "# fMRI Lab: Day 2 -- Validation\n\n## HYPOTHESIS-DRIVEN (Depression)\n\nWelcome back. Today you will find out whether your pre-registered hypothesis holds up in an independent dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387f6da5fd7",
   "metadata": {},
   "source": [
    "---\n\n## Part 1: Setup and Re-enter Your Pre-Registered Analysis\n\nRun the setup cell below. **Before clicking Run**, fill in the three variables at the top of the cell to match your Day 1 pre-registration. These must match exactly what you committed to on Day 1 -- do not change them based on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813a362c10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n# FILL IN YOUR PRE-REGISTERED CHOICES FROM DAY 1 BEFORE RUNNING\n# =====================================================================\n\ncovariates = None        # Example: ['Age', 'Sex'] or None\noutlier_threshold = None # Example: 2 or 3, or None for no removal\nsubgroup = None          # Example: {'Sex': 0} or None for full sample\n\n# =====================================================================\n\nimport subprocess, sys\nsubprocess.check_call([sys.executable, '-m', 'pip', 'install', 'nilearn', '-q'])\n\nimport os, urllib.request\nbase_url = 'https://raw.githubusercontent.com/cmahlen/python-stats-demo/main/'\nfiles_needed = [\n    'lab_helpers.py', 'atlas_labels.txt', 'data/roi_mni_coords.npy',\n    'data/depression_discovery.npz', 'data/depression_validation.npz',\n]\nos.makedirs('data', exist_ok=True)\nfor f in files_needed:\n    if not os.path.exists(f):\n        urllib.request.urlretrieve(base_url + f, f)\n\nimport lab_helpers as helpers\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr\nfrom statsmodels.stats.multitest import multipletests\n\nhelpers.load_dataset('depression', 'discovery')\nbehavior = helpers.get_behavior()\n\n# Re-run your hypothesis test with your pre-registered choices\nall_results = helpers.test_network_edges('Salience',\n                                          covariates=covariates,\n                                          exclude_outliers=outlier_threshold,\n                                          subgroup=subgroup)\nhyp_mask = (\n    (all_results['ROI_A'].str.contains('SalVentAttnA') & all_results['ROI_A'].str.contains('Ins') &\n     all_results['ROI_B'].str.contains('SalVentAttnA') & all_results['ROI_B'].str.contains('FrMed')) |\n    (all_results['ROI_B'].str.contains('SalVentAttnA') & all_results['ROI_B'].str.contains('Ins') &\n     all_results['ROI_A'].str.contains('SalVentAttnA') & all_results['ROI_A'].str.contains('FrMed'))\n)\nhyp_results = all_results[hyp_mask].reset_index(drop=True)\nn_hyp_tests = len(hyp_results)\n\nalpha = 0.05\nreject, p_corrected, _, _ = multipletests(hyp_results['p'], alpha=alpha, method='fdr_bh')\nhyp_results['p_corrected'] = p_corrected\nhyp_results['significant_fdr'] = reject\nn_significant = hyp_results['significant_fdr'].sum()\n\ndef _short_name(roi):\n    if '-' in roi:\n        return roi\n    parts = roi.split('_')\n    if len(parts) >= 4:\n        return '_'.join(parts[1:-1])\n    return roi\n\nprint(f\"Reloaded! {n_significant} significant Salience Ins-FrMed edges (FDR corrected)\")\nprint(f\"\\nYour hypothesis edges:\")\nprint(hyp_results[['ROI_A', 'ROI_B', 'r', 'p', 'p_corrected', 'significant_fdr']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflection",
   "metadata": {},
   "source": [
    "---\n\n## Part 2: Reflection Questions\n\nBefore we test your findings in the validation data, think about these questions:\n\n1. **How confident are you that your findings will replicate?**\n   - What would make you more or less confident?\n\n2. **What could go wrong?**\n   - Even with FDR correction, could your results still be false positives?\n\n3. **How is your approach different from the exploratory group's?**\n   - How many tests did you run compared to them?\n   - Did you choose your analysis plan before or after seeing the results?\n\n4. **Effect sizes**\n   - How strong were the correlations you found?\n   - Are these practically meaningful for understanding depression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation-header",
   "metadata": {},
   "source": [
    "---\n\n## Part 3: Validation\n\n### Test your findings in the independent validation set\n\nTrue effects should replicate in new data. You will now re-run your **exact pre-registered analysis** on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the validation dataset\nhelpers.load_dataset('depression', 'validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation-test-intro",
   "metadata": {},
   "source": [
    "The cell below tests each of your hypothesis edges in the validation dataset and compares the results to your discovery findings. An edge \"replicates\" if it is significant in the same direction in both datasets. You do not need to understand every line of code -- just run the cell and read the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n\n# Test ALL hypothesis edges in validation using the SAME analysis choices\nval_behavior = helpers.get_behavior()\nval_outcome = val_behavior['PHQ9'].values\n\nvalidation_results = []\nfor _, row in hyp_results.iterrows():\n    edge_vals = helpers.get_edge(row['ROI_A'], row['ROI_B'])\n    r_val, p_val = pearsonr(edge_vals, val_outcome)\n\n    sig_disc = row['significant_fdr']\n    same_direction = (r_val * row['r']) > 0\n    if sig_disc:\n        if p_val < 0.05 and same_direction:\n            replicated = 'YES'\n        elif p_val < 0.05 and not same_direction:\n            replicated = 'FLIPPED'\n        else:\n            replicated = 'NO'\n    else:\n        replicated = 'N/A'\n\n    validation_results.append({\n        'ROI_A': row['ROI_A'],\n        'ROI_B': row['ROI_B'],\n        'Disc_r': row['r'],\n        'Disc_p': row['p'],\n        'Sig_Disc': sig_disc,\n        'Val_r': r_val,\n        'Val_p': p_val,\n        'Replicated': replicated,\n    })\n\nval_df = pd.DataFrame(validation_results)\n\nprint('=' * 80)\nprint('VALIDATION RESULTS: Salience Ins-FrMed Edges')\nprint('=' * 80)\nprint('Validation uses uncorrected p < 0.05 + same direction as replication criterion\\n')\n\ndisplay_df = val_df[['ROI_A', 'ROI_B', 'Disc_r', 'Disc_p', 'Sig_Disc', 'Val_r', 'Val_p', 'Replicated']]\ndisplay_df.index = range(1, len(display_df) + 1)\nprint(display_df.to_string())\n\nif n_significant > 0:\n    n_rep = (val_df['Replicated'] == 'YES').sum()\n    print(f\"\\n{'=' * 80}\")\n    print(f'REPLICATION: {n_rep}/{n_significant} significant findings replicated')\n    print(f\"{'=' * 80}\")\n\nn_flipped = (val_df['Replicated'] == 'FLIPPED').sum()\nif n_flipped > 0:\n    print(f'WARNING: {n_flipped} finding(s) were significant but in the OPPOSITE direction!')\n    print('A flipped direction means the effect is not replicating -- it is noise.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation-viz-intro",
   "metadata": {},
   "source": [
    "The cell below creates side-by-side scatter plots comparing your discovery and validation results. The left plot shows the discovery set; the right plot shows the validation set. If the finding replicated, both plots should show a similar pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-replication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side visualization for significant finding(s)\nif n_significant > 0:\n    sig_rows = val_df[val_df['Sig_Disc'] == True]\n\n    for _, row in sig_rows.iterrows():\n        fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n\n        # Discovery\n        helpers.load_dataset('depression', 'discovery')\n        disc_edge = helpers.get_edge(row['ROI_A'], row['ROI_B'])\n        disc_out = helpers.get_behavior()['PHQ9'].values\n        r_d, p_d = pearsonr(disc_edge, disc_out)\n\n        axes[0].scatter(disc_edge, disc_out, alpha=0.5, color='steelblue')\n        z = np.polyfit(disc_edge, disc_out, 1)\n        x_line = np.linspace(disc_edge.min(), disc_edge.max(), 100)\n        axes[0].plot(x_line, np.polyval(z, x_line), color='coral', linewidth=2)\n        axes[0].set_xlabel('Functional Connectivity')\n        axes[0].set_ylabel('PHQ9')\n        axes[0].set_title(f'Discovery Set\\nr = {r_d:.3f}, p = {p_d:.2e}')\n\n        # Validation\n        helpers.load_dataset('depression', 'validation')\n        val_edge = helpers.get_edge(row['ROI_A'], row['ROI_B'])\n        val_out = helpers.get_behavior()['PHQ9'].values\n        r_v, p_v = pearsonr(val_edge, val_out)\n\n        axes[1].scatter(val_edge, val_out, alpha=0.5, color='mediumseagreen')\n        z = np.polyfit(val_edge, val_out, 1)\n        x_line = np.linspace(val_edge.min(), val_edge.max(), 100)\n        axes[1].plot(x_line, np.polyval(z, x_line), color='coral', linewidth=2)\n        axes[1].set_xlabel('Functional Connectivity')\n        axes[1].set_ylabel('PHQ9')\n        axes[1].set_title(f'Validation Set\\nr = {r_v:.3f}, p = {p_v:.2e}')\n\n        short_a = _short_name(row['ROI_A'])\n        short_b = _short_name(row['ROI_B'])\n        fig.suptitle(f'{short_a} <-> {short_b}: Discovery vs Validation', fontsize=13)\n        plt.tight_layout()\n        plt.show()\n\n        if row['Replicated'] == 'YES':\n            print(f'REPLICATED in validation set!')\n        else:\n            print(f'Did not replicate in validation set.')\n        print()\nelse:\n    print('No significant findings to visualize.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "your-turn-nonsig",
   "metadata": {},
   "source": [
    "---\n\n## Part 5: Your Turn -- Test a Non-Significant Edge\n\nPick one edge from your hypothesis set that was **not** significant in discovery. Does it look any different in validation? This helps build intuition about what \"noise\" looks like compared to a real effect.\n\n<details>\n<summary>Hint: Example code</summary>\n\n```python\n# Pick the last (least significant) edge from your hypothesis results\nnonsig = hyp_results.iloc[-1]\nhelpers.load_dataset('depression', 'discovery')\nhelpers.plot_edge(nonsig['ROI_A'], nonsig['ROI_B'], 'PHQ9')\n```\n</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "your-turn-nonsig-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Turn: test a non-significant edge here\n"
   ]
  }
 ]
}