{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Why Python? A Hands-On Comparison with JASP\n",
    "\n",
    "You just learned how to run statistical tests in JASP. Now let's do the **exact same tests** in Python and see what we gain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "why-python",
   "metadata": {},
   "source": [
    "## Why bother with code when JASP exists?\n",
    "\n",
    "JASP is excellent for learning statistical concepts and running quick analyses. Python offers additional capabilities that become valuable as your research grows:\n",
    "\n",
    "| | JASP (GUI) | Python (Code) |\n",
    "|---|---|---|\n",
    "| **Reproducibility** | Need to remember which buttons you clicked | Save code, run again anytime â€” same result guaranteed |\n",
    "| **Automation** | Analyze 50 brain regions? Click 50 times | `for region in regions: analyze(region)` â€” done in seconds |\n",
    "| **Flexibility** | Limited to built-in options | Create any analysis you can describe |\n",
    "| **Transparency** | Results appear; steps are hidden | You control (and can inspect) each step |\n",
    "| **Sharing** | Colleague needs JASP installed | Share a .py file or notebook â€” runs in any Python environment |\n",
    "| **Scale** | Great for one dataset at a time | Process thousands of files overnight |\n",
    "\n",
    "**Bottom line:** JASP is great for learning concepts and quick checks. Python gives you power and flexibility for real research.\n",
    "\n",
    "Let's prove it. We'll replicate everything you just did in JASP â€” and then go further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setup and Loading Data\n",
    "\n",
    "First, let's load the Python libraries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import the libraries we'll use\n",
    "import pandas as pd          # For working with data tables\n",
    "import numpy as np           # For numerical operations\n",
    "import matplotlib.pyplot as plt  # For creating plots\n",
    "import scipy.stats as stats  # For statistical tests\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-context",
   "metadata": {},
   "source": [
    "### The Study\n",
    "\n",
    "A pharmacology experiment examining how dopaminergic drugs affect locomotor activity in rodents.\n",
    "\n",
    "**Dependent Variable**: Number of squares entered in an open-field test (measure of locomotion)\n",
    "\n",
    "**Independent Variables** (8 drug treatment groups):\n",
    "- **Control**: No drug (baseline)\n",
    "- **Amph**: Amphetamine (dopamine releaser â†’ increases locomotion)\n",
    "- **Res only**: Reserpine (depletes dopamine â†’ decreases locomotion)\n",
    "- **Res+Amph**: Can amphetamine overcome reserpine's effects?\n",
    "- **Res+MT**: Reserpine + alpha-methyltyrosine (blocks dopamine synthesis)\n",
    "- **Res+MT+Amph**: Triple combination\n",
    "- **Res+MT+DOPA**: L-DOPA (dopamine precursor) to restore function?\n",
    "- **Res+MT+Amph+DOPA**: Maximum restoration attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-anova-header",
   "metadata": {},
   "source": [
    "### Loading Data: The Peek-Then-Use Pattern\n",
    "\n",
    "**Good habit:** Always look at your data before analyzing it! This helps you catch problems early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-anova-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load the ANOVA dataset\n",
    "anova_df = pd.read_excel('class_data_undergrad.xlsx')\n",
    "\n",
    "# Step 3: ALWAYS peek at your data first!\n",
    "anova_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Check the structure of your data\n",
    "print(\"DATA QUALITY CHECK\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Number of rows: {len(anova_df)}\")\n",
    "print(f\"Number of columns: {len(anova_df.columns)}\")\n",
    "print(f\"\\nColumn names: {list(anova_df.columns)}\")\n",
    "print(f\"\\nMissing values: {anova_df.isna().sum().sum()}\")\n",
    "print(f\"\\nGroups in the data: {anova_df['Group'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-viz-intro",
   "metadata": {},
   "source": [
    "### Visualize Before Analyzing!\n",
    "\n",
    "**Good habit:** Always plot your data before running statistics. Visualizations help you:\n",
    "- Spot outliers or data entry errors\n",
    "- See the pattern before confirming it with numbers\n",
    "- Choose the right statistical test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Visualize the data â€” immediate payoff!\n",
    "plt.figure(figsize=(12, 5))\n",
    "anova_df.boxplot(column='Squares entered', by='Group', rot=45)\n",
    "plt.title('Locomotor Activity by Treatment Group')\n",
    "plt.suptitle('')  # Remove the automatic title pandas adds\n",
    "plt.ylabel('Squares Entered')\n",
    "plt.xlabel('Treatment Group')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"What patterns do you notice? Which groups look different from Control?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descriptives",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Descriptive statistics by group\n",
    "# This is what JASP calls \"Descriptives\"\n",
    "descriptives = anova_df.groupby('Group')['Squares entered'].agg(['count', 'mean', 'std']).round(2)\n",
    "descriptives.columns = ['N', 'Mean', 'SD']\n",
    "\n",
    "# Order groups logically for pharmacological interpretation\n",
    "group_order = ['Control', 'Amph', 'Res only', 'Res+Amph', 'Res+MT', 'Res+MT+Amph', 'Res+MT+DOPA', 'Res+MT+Amph+DOPA']\n",
    "descriptives = descriptives.reindex(group_order)\n",
    "\n",
    "print(\"Descriptive Statistics by Treatment Group\")\n",
    "print(\"=\" * 50)\n",
    "print(descriptives.to_string())\n",
    "print(\"\\nCompare with your JASP output!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-1-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ðŸŽ¯ Your Turn: Load the T-Test Data\n",
    "\n",
    "Now it's your turn! Load the file `Old and New Data Set t-tests.xlsx` (sheet: `Data Set 1`) and look at the first few rows with `.head()`.\n",
    "\n",
    "**Hint:** You'll need to use `pd.read_excel()` with a `sheet_name` argument.\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "ttest_df1 = pd.read_excel('Old and New Data Set t-tests.xlsx',\n",
    "                          sheet_name='Data Set 1',\n",
    "                          skiprows=1)  # Skip the empty first row\n",
    "ttest_df1.head()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-1-space",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ttest-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: T-Tests\n",
    "\n",
    "We'll analyze both datasets from your JASP session and compare independent vs. paired t-tests.\n",
    "\n",
    "### Data Set 1: Loading and Cleaning\n",
    "\n",
    "This Excel file has a quirky structure â€” let's see what we're dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-ttest-data1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the t-test data and peek at it\n",
    "df1_raw = pd.read_excel('Old and New Data Set t-tests.xlsx', sheet_name='Data Set 1', header=None)\n",
    "\n",
    "print(\"Raw data (first 18 rows):\")\n",
    "print(df1_raw.head(18))\n",
    "print(\"\\n... and the last few rows:\")\n",
    "print(df1_raw.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleaning-explanation",
   "metadata": {},
   "source": [
    "**What we see:** The actual data starts at row 2, and there are summary statistics at the bottom that we need to exclude.\n",
    "\n",
    "Let's extract just the data we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-ttest-data1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the New and Old columns (skip header row, get numeric values only)\n",
    "new_data1 = pd.to_numeric(df1_raw.iloc[2:, 0], errors='coerce').dropna().values\n",
    "old_data1 = pd.to_numeric(df1_raw.iloc[2:, 1], errors='coerce').dropna().values\n",
    "\n",
    "print(\"T-Test Data Set 1 (Equal Variances):\")\n",
    "print(f\"  New group: n={len(new_data1)}, mean={new_data1.mean():.2f}, SD={new_data1.std(ddof=1):.2f}\")\n",
    "print(f\"  Old group: n={len(old_data1)}, mean={old_data1.mean():.2f}, SD={old_data1.std(ddof=1):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-before-ttest",
   "metadata": {},
   "source": [
    "### Visualize Before Testing\n",
    "\n",
    "Before running the t-test, let's see what the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "histogram-ttest1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the two groups\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(new_data1, bins=8, alpha=0.7, label='New', color='steelblue', edgecolor='black')\n",
    "axes[0].hist(old_data1, bins=8, alpha=0.7, label='Old', color='coral', edgecolor='black')\n",
    "axes[0].set_xlabel('Score')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Data Set 1: Overlapping Histograms')\n",
    "axes[0].legend()\n",
    "\n",
    "# Boxplot\n",
    "axes[1].boxplot([new_data1, old_data1], labels=['New', 'Old'])\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_title('Data Set 1: Side-by-Side Boxplots')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The groups look fairly similar in spread (variance). Is the New group higher?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-ttest-intro",
   "metadata": {},
   "source": [
    "### Independent Samples T-Test\n",
    "\n",
    "**Question**: Is there a difference between the New and Old groups?\n",
    "\n",
    "In JASP: T-Tests â†’ Independent Samples T-Test â†’ drag variables â†’ click options...\n",
    "\n",
    "In Python: One line of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-ttest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent samples t-test (assumes equal variances)\n",
    "t_stat, p_value = stats.ttest_ind(new_data1, old_data1)\n",
    "\n",
    "print(\"INDEPENDENT SAMPLES T-TEST (Data Set 1)\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"New group mean: {new_data1.mean():.2f} (n={len(new_data1)})\")\n",
    "print(f\"Old group mean: {old_data1.mean():.2f} (n={len(old_data1)})\")\n",
    "print(f\"\\nt = {t_stat:.3f}, p = {p_value:.4f}\")\n",
    "print(f\"\\nSignificant at Î±=0.05? {'Yes' if p_value < 0.05 else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paired-vs-independent",
   "metadata": {},
   "source": [
    "### Teaching Moment: Independent vs. Paired T-Tests\n",
    "\n",
    "What happens if we run the same data as a **paired** t-test? The results will differ!\n",
    "\n",
    "- **Independent t-test**: Assumes the groups are unrelated (different subjects)\n",
    "- **Paired t-test**: Assumes each observation in one group is matched to one in the other\n",
    "\n",
    "---\n",
    "\n",
    "âš ï¸ **Important Warning:** The following is a \"what happens if we use the wrong test\" demonstration. A paired t-test requires **true subject-level pairing** (e.g., the same person measured before and after treatment). We're artificially pairing unrelated observations here to show how results change â€” **never do this with real data!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paired-ttest-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For paired t-test, we need equal sample sizes\n",
    "# Truncate to the smaller group (this is artificial pairing - just for demonstration!)\n",
    "n_min = min(len(new_data1), len(old_data1))\n",
    "new_paired = new_data1[:n_min]\n",
    "old_paired = old_data1[:n_min]\n",
    "\n",
    "# Independent t-test\n",
    "t_ind, p_ind = stats.ttest_ind(new_paired, old_paired)\n",
    "\n",
    "# Paired t-test (ttest_rel = related samples)\n",
    "t_paired, p_paired = stats.ttest_rel(new_paired, old_paired)\n",
    "\n",
    "print(\"COMPARISON: Independent vs. Paired T-Tests\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Using first {n_min} observations from each group\\n\")\n",
    "print(f\"Independent t-test: t = {t_ind:.3f}, p = {p_ind:.4f}\")\n",
    "print(f\"Paired t-test:      t = {t_paired:.3f}, p = {p_paired:.4f}\")\n",
    "print(\"\\n** Key insight: The p-values differ! **\")\n",
    "print(\"\\nWhen to use each:\")\n",
    "print(\"  - Independent: Different subjects in each group (e.g., treatment vs control)\")\n",
    "print(\"  - Paired: Same subjects measured twice (e.g., before vs after)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-2-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ðŸŽ¯ Your Turn: Create a Histogram\n",
    "\n",
    "Create a histogram showing just the `new_data1` values. Use `plt.hist()` with:\n",
    "- `bins=10`\n",
    "- `color='steelblue'`\n",
    "- `edgecolor='black'`\n",
    "\n",
    "Don't forget to add a title with `plt.title()` and show the plot with `plt.show()`!\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "plt.hist(new_data1, bins=10, color='steelblue', edgecolor='black')\n",
    "plt.title('Distribution of New Group Scores')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-2-space",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unequal-variance-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Data Set 2: Unequal Variances â€” When Standard T-Tests Fail\n",
    "\n",
    "**Problem**: The standard t-test assumes equal variances. But sometimes groups have very different spreads. Let's load Data Set 2 and see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data Set 2 (note: sheet name has trailing space in the Excel file)\n",
    "df2_raw = pd.read_excel('Old and New Data Set t-tests.xlsx', sheet_name='Data Set 2 ', header=None)\n",
    "new_data2 = pd.to_numeric(df2_raw.iloc[2:, 0], errors='coerce').dropna().values\n",
    "old_data2 = pd.to_numeric(df2_raw.iloc[2:, 1], errors='coerce').dropna().values\n",
    "\n",
    "print(\"T-Test Data Set 2 (Unequal Variances):\")\n",
    "print(f\"  New group: n={len(new_data2)}, mean={new_data2.mean():.2f}, SD={new_data2.std(ddof=1):.2f}\")\n",
    "print(f\"  Old group: n={len(old_data2)}, mean={old_data2.mean():.2f}, SD={old_data2.std(ddof=1):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-variance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZE the variance difference â€” much clearer than numbers!\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "axes[0].hist(new_data2, bins=8, alpha=0.7, label='New', color='steelblue', edgecolor='black')\n",
    "axes[0].hist(old_data2, bins=8, alpha=0.7, label='Old', color='coral', edgecolor='black')\n",
    "axes[0].set_xlabel('Score')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Data Set 2: Overlapping Histograms')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].boxplot([new_data2, old_data2], labels=['New', 'Old'])\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_title('Data Set 2: Side-by-Side Boxplots')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how the 'New' group is MUCH more spread out than the 'Old' group!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-variance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check variances numerically\n",
    "var_new = new_data2.var(ddof=1)\n",
    "var_old = old_data2.var(ddof=1)\n",
    "\n",
    "print(\"Variance Check (Data Set 2):\")\n",
    "print(f\"  New group: variance = {var_new:.2f}, SD = {np.sqrt(var_new):.2f}\")\n",
    "print(f\"  Old group: variance = {var_old:.2f}, SD = {np.sqrt(var_old):.2f}\")\n",
    "print(f\"\\nVariance ratio: {var_new/var_old:.2f}x difference!\")\n",
    "\n",
    "# Levene's test for equality of variances\n",
    "lev_stat, lev_p = stats.levene(new_data2, old_data2)\n",
    "print(f\"\\nLevene's test: F = {lev_stat:.3f}, p = {lev_p:.4f}\")\n",
    "if lev_p > 0.05:\n",
    "    print(\"Equal variances? Yes (p > 0.05)\")\n",
    "else:\n",
    "    print(\"Equal variances? No (p < 0.05) â€” use Welch's t-test!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welch-ttest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare standard vs Welch's t-test\n",
    "t_standard, p_standard = stats.ttest_ind(new_data2, old_data2, equal_var=True)\n",
    "t_welch, p_welch = stats.ttest_ind(new_data2, old_data2, equal_var=False)  # Welch's\n",
    "\n",
    "print(\"T-TEST COMPARISON (Data Set 2 - Unequal Variances)\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"New group: mean = {new_data2.mean():.2f}, SD = {new_data2.std(ddof=1):.2f}\")\n",
    "print(f\"Old group: mean = {old_data2.mean():.2f}, SD = {old_data2.std(ddof=1):.2f}\")\n",
    "print(f\"\\nStandard t-test (assumes equal var): t = {t_standard:.3f}, p = {p_standard:.4f}\")\n",
    "print(f\"Welch's t-test  (unequal var OK):    t = {t_welch:.3f}, p = {p_welch:.4f}\")\n",
    "print(\"\\n** When variances are unequal, use Welch's t-test! **\")\n",
    "print(\"In Python: stats.ttest_ind(a, b, equal_var=False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anova-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: One-Way ANOVA\n",
    "\n",
    "**Question**: Does locomotor activity differ across the 8 drug treatment groups?\n",
    "\n",
    "In JASP: ANOVA â†’ drag Dependent Variable â†’ drag Fixed Factors â†’ check Post Hoc...\n",
    "\n",
    "In Python: First visualize, then test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anova-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always visualize before running stats!\n",
    "plt.figure(figsize=(12, 5))\n",
    "anova_df.boxplot(column='Squares entered', by='Group', rot=45)\n",
    "plt.title('What patterns do you see?')\n",
    "plt.suptitle('')  # Remove the automatic pandas title\n",
    "plt.ylabel('Squares Entered')\n",
    "plt.xlabel('Treatment Group')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Look at the plot: Which groups appear different from Control?\")\n",
    "print(\"Which groups have near-zero locomotion?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oneway-anova",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the groups for analysis\n",
    "groups = {}\n",
    "for group_name in group_order:\n",
    "    groups[group_name] = anova_df[anova_df['Group'] == group_name]['Squares entered'].values\n",
    "\n",
    "# One-way ANOVA\n",
    "F_stat, p_value = stats.f_oneway(*groups.values())\n",
    "\n",
    "# Calculate degrees of freedom\n",
    "k = len(groups)  # number of groups\n",
    "N = len(anova_df)  # total sample size\n",
    "df_between = k - 1\n",
    "df_within = N - k\n",
    "\n",
    "# Format p-value properly (don't hard-code!)\n",
    "if p_value < 0.001:\n",
    "    p_str = \"p < .001\"\n",
    "else:\n",
    "    p_str = f\"p = {p_value:.4f}\"\n",
    "\n",
    "print(\"ONE-WAY ANOVA: Locomotor Activity by Treatment Group\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"\\nF({df_between},{df_within}) = {F_stat:.2f}, {p_str}\")\n",
    "print(\"\\n** Compare with JASP: F(7,102) = 82.00, p < .001 **\")\n",
    "print(\"\\nGroup means (squares entered):\")\n",
    "for group_name in group_order:\n",
    "    mean = groups[group_name].mean()\n",
    "    print(f\"  {group_name:20s}: {mean:6.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posthoc-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Post-Hoc Comparisons (Tukey HSD)\n",
    "\n",
    "The ANOVA tells us groups differ, but **which** groups differ from each other?\n",
    "\n",
    "With 8 groups, we have 8Ã—7/2 = **28 pairwise comparisons**.\n",
    "\n",
    "### Why Not Just Run 28 T-Tests?\n",
    "\n",
    "You might think: \"If I want to compare 8 groups, I'll just run all pairwise t-tests!\"\n",
    "\n",
    "**The problem:** With 28 comparisons at Î± = 0.05, you expect ~1-2 false positives by chance alone. This is called the **multiple comparisons problem**.\n",
    "\n",
    "**Solutions:**\n",
    "1. **Tukey HSD** (what we'll use) â€” adjusts p-values to control family-wise error rate\n",
    "2. **Bonferroni correction** â€” multiply each p-value by the number of tests (more conservative)\n",
    "\n",
    "Let's use Tukey HSD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tukey-hsd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import tukey_hsd\n",
    "\n",
    "# Tukey HSD for all pairwise comparisons\n",
    "result = tukey_hsd(*[groups[g] for g in group_order])\n",
    "\n",
    "print(\"TUKEY HSD POST-HOC COMPARISONS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Comparison':<45} {'Mean Diff':>10} {'p-value':>10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Store significant and non-significant comparisons\n",
    "significant_pairs = []\n",
    "nonsig_pairs = []\n",
    "\n",
    "for i in range(len(group_order)):\n",
    "    for j in range(i+1, len(group_order)):\n",
    "        g1, g2 = group_order[i], group_order[j]\n",
    "        mean_diff = groups[g1].mean() - groups[g2].mean()\n",
    "        p_val = result.pvalue[i, j]\n",
    "        \n",
    "        sig_marker = \"***\" if p_val < 0.001 else \"**\" if p_val < 0.01 else \"*\" if p_val < 0.05 else \"\"\n",
    "        comparison = f\"{g1} vs {g2}\"\n",
    "        print(f\"{comparison:<45} {mean_diff:>10.2f} {p_val:>10.3f} {sig_marker}\")\n",
    "        \n",
    "        if p_val < 0.05:\n",
    "            significant_pairs.append((g1, g2, mean_diff, p_val))\n",
    "        else:\n",
    "            nonsig_pairs.append((g1, g2, mean_diff, p_val))\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(f\"Significant comparisons: {len(significant_pairs)} of 28\")\n",
    "print(\"*** p < .001, ** p < .01, * p < .05\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "key-comparisons",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlight key pharmacological findings â€” using COMPUTED p-values from Tukey results\n",
    "print(\"\\nKEY PHARMACOLOGICAL FINDINGS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Helper function to format p-values\n",
    "def format_p(p):\n",
    "    if p < 0.001:\n",
    "        return \"p < .001\"\n",
    "    else:\n",
    "        return f\"p = {p:.3f}\"\n",
    "\n",
    "# Get p-values from Tukey result matrix\n",
    "# Indices match group_order: Control=0, Amph=1, Res only=2, etc.\n",
    "p_amph_ctrl = result.pvalue[0, 1]  # Control vs Amph\n",
    "p_res_ctrl = result.pvalue[0, 2]   # Control vs Res only\n",
    "p_resamph_ctrl = result.pvalue[0, 3]  # Control vs Res+Amph\n",
    "p_final_ctrl = result.pvalue[0, 7]  # Control vs Res+MT+Amph+DOPA\n",
    "\n",
    "print(\"\\n1. Amphetamine INCREASES locomotion:\")\n",
    "amph_vs_ctrl = groups['Amph'].mean() - groups['Control'].mean()\n",
    "print(f\"   Amph vs Control: +{amph_vs_ctrl:.1f} squares ({format_p(p_amph_ctrl)})\")\n",
    "\n",
    "print(\"\\n2. Reserpine ABOLISHES locomotion:\")\n",
    "res_vs_ctrl = groups['Res only'].mean() - groups['Control'].mean()\n",
    "print(f\"   Res only vs Control: {res_vs_ctrl:.1f} squares ({format_p(p_res_ctrl)})\")\n",
    "\n",
    "print(\"\\n3. Amphetamine PARTIALLY restores function after reserpine:\")\n",
    "print(f\"   Res+Amph mean: {groups['Res+Amph'].mean():.1f} (vs Control: {format_p(p_resamph_ctrl)})\")\n",
    "\n",
    "print(\"\\n4. MT blocks the amphetamine restoration:\")\n",
    "print(f\"   Res+MT+Amph mean: {groups['Res+MT+Amph'].mean():.1f} (essentially zero)\")\n",
    "\n",
    "print(\"\\n5. L-DOPA + Amph can restore function even with MT:\")\n",
    "print(f\"   Res+MT+Amph+DOPA mean: {groups['Res+MT+Amph+DOPA'].mean():.1f} (vs Control: {format_p(p_final_ctrl)})\")\n",
    "\n",
    "print(\"\\n** These findings demonstrate the dopamine hypothesis of locomotion! **\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-3-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ðŸŽ¯ Your Turn: Run a T-Test\n",
    "\n",
    "Run an independent samples t-test comparing the `Control` group to the `Res only` group.\n",
    "\n",
    "Use `stats.ttest_ind()` with the data from the `groups` dictionary we created earlier.\n",
    "\n",
    "**Hint:** Access the data with `groups['Control']` and `groups['Res only']`\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "t_stat, p_val = stats.ttest_ind(groups['Control'], groups['Res only'])\n",
    "print(f\"t = {t_stat:.3f}, p = {p_val:.4f}\")\n",
    "print(f\"Control mean: {groups['Control'].mean():.2f}\")\n",
    "print(f\"Res only mean: {groups['Res only'].mean():.2f}\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-3-space",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advantage-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Things Python Can Do That JASP Can't\n",
    "\n",
    "### Advantage 1: Automation\n",
    "\n",
    "What if you needed to run pairwise comparisons for multiple outcome measures? In JASP, you'd click through menus dozens of times. In Python, it's a loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automation-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all 28 pairwise t-tests programmatically\n",
    "print(\"AUTOMATED PAIRWISE T-TESTS (all 28 comparisons)\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "results_list = []\n",
    "for i in range(len(group_order)):\n",
    "    for j in range(i+1, len(group_order)):\n",
    "        g1, g2 = group_order[i], group_order[j]\n",
    "        t, p = stats.ttest_ind(groups[g1], groups[g2])\n",
    "        results_list.append({\n",
    "            'Group 1': g1,\n",
    "            'Group 2': g2,\n",
    "            't': t,\n",
    "            'p': p,\n",
    "            'significant': p < 0.05\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results_list)\n",
    "print(f\"\\nRan {len(results_df)} t-tests in a loop.\")\n",
    "print(f\"Significant results (uncorrected): {results_df['significant'].sum()}\")\n",
    "\n",
    "# Apply Bonferroni correction\n",
    "results_df['p_bonf'] = results_df['p'] * len(results_df)\n",
    "results_df['p_bonf'] = results_df['p_bonf'].clip(upper=1.0)  # Cap at 1.0\n",
    "results_df['sig_bonf'] = results_df['p_bonf'] < 0.05\n",
    "print(f\"Significant after Bonferroni correction: {results_df['sig_bonf'].sum()}\")\n",
    "\n",
    "print(\"\\nâš ï¸ Note: Running many uncorrected t-tests inflates false positives!\")\n",
    "print(\"   Always use Tukey HSD or Bonferroni correction for post-hoc comparisons.\")\n",
    "print(\"\\n--> In JASP, you'd click through 28 separate comparisons.\")\n",
    "print(\"    In Python, 8 lines of code.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-header",
   "metadata": {},
   "source": [
    "### Advantage 2: Custom Visualizations\n",
    "\n",
    "JASP gives you canned plots. Python gives you full control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bar-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publication-quality bar plot with error bars\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "means = [groups[g].mean() for g in group_order]\n",
    "sems = [groups[g].std() / np.sqrt(len(groups[g])) for g in group_order]\n",
    "\n",
    "# Color code by drug effect\n",
    "colors = ['#4DAF4A',   # Control - green\n",
    "          '#E41A1C',   # Amph - red (stimulant)\n",
    "          '#377EB8',   # Res only - blue (depleted)\n",
    "          '#FF7F00',   # Res+Amph - orange (partial restoration)\n",
    "          '#377EB8',   # Res+MT - blue\n",
    "          '#377EB8',   # Res+MT+Amph - blue\n",
    "          '#377EB8',   # Res+MT+DOPA - blue\n",
    "          '#FF7F00']   # Res+MT+Amph+DOPA - orange (restoration)\n",
    "\n",
    "bars = ax.bar(range(len(group_order)), means, yerr=sems, \n",
    "              color=colors, edgecolor='black', capsize=4, alpha=0.8)\n",
    "\n",
    "ax.set_xticks(range(len(group_order)))\n",
    "ax.set_xticklabels(group_order, rotation=45, ha='right', fontsize=10)\n",
    "ax.set_ylabel('Squares Entered (Â± SEM)', fontsize=12)\n",
    "ax.set_xlabel('Treatment Group', fontsize=12)\n",
    "\n",
    "# Use computed F and p for title\n",
    "if p_value < 0.001:\n",
    "    title_p = \"p < .001\"\n",
    "else:\n",
    "    title_p = f\"p = {p_value:.3f}\"\n",
    "ax.set_title(f'Locomotor Activity by Pharmacological Treatment\\nF({df_between},{df_within}) = {F_stat:.2f}, {title_p}', fontsize=14)\n",
    "\n",
    "# Add horizontal line at Control mean for reference\n",
    "ax.axhline(y=groups['Control'].mean(), color='gray', linestyle='--', alpha=0.5, label='Control baseline')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nColor key: Green = Control, Red = Stimulant effect, Blue = Depleted, Orange = Restored\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bootstrap-header",
   "metadata": {},
   "source": [
    "### Advantage 3: Bootstrap Confidence Intervals\n",
    "\n",
    "This isn't even an option in JASP's menus. In Python, it's straightforward.\n",
    "\n",
    "### Why Bootstrap?\n",
    "\n",
    "Traditional statistics assume your data follows a specific distribution (usually normal). But what if it doesn't? Or what if you have a small sample?\n",
    "\n",
    "**Bootstrapping** lets you estimate uncertainty without those assumptions:\n",
    "1. Resample your data (with replacement) thousands of times\n",
    "2. Calculate your statistic each time\n",
    "3. The spread of results IS your confidence interval\n",
    "\n",
    "This is especially useful when:\n",
    "- Sample sizes are small\n",
    "- Data is skewed or has outliers\n",
    "- You want to be confident your results aren't flukes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bootstrap-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap CI for Amph vs Control difference\n",
    "amph = groups['Amph']\n",
    "control = groups['Control']\n",
    "\n",
    "n_bootstrap = 10000\n",
    "boot_diffs = []\n",
    "\n",
    "np.random.seed(42)  # For reproducibility\n",
    "for _ in range(n_bootstrap):\n",
    "    boot_amph = np.random.choice(amph, size=len(amph), replace=True)\n",
    "    boot_ctrl = np.random.choice(control, size=len(control), replace=True)\n",
    "    boot_diffs.append(boot_amph.mean() - boot_ctrl.mean())\n",
    "\n",
    "boot_diffs = np.array(boot_diffs)\n",
    "\n",
    "print(\"BOOTSTRAP ANALYSIS (10,000 resamples)\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"Amphetamine vs Control difference\")\n",
    "print(f\"\\nObserved difference: {amph.mean() - control.mean():.1f} squares\")\n",
    "print(f\"Bootstrap mean:      {np.mean(boot_diffs):.1f} squares\")\n",
    "print(f\"95% CI: [{np.percentile(boot_diffs, 2.5):.1f}, {np.percentile(boot_diffs, 97.5):.1f}]\")\n",
    "print(f\"\\nP(Amph > Control): {(boot_diffs > 0).mean():.1%}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(boot_diffs, bins=50, edgecolor='black', alpha=0.7, color='#E41A1C')\n",
    "plt.axvline(0, color='black', linewidth=2, linestyle='--', label='No difference')\n",
    "plt.axvline(np.percentile(boot_diffs, 2.5), color='blue', linewidth=2, linestyle=':', label='95% CI')\n",
    "plt.axvline(np.percentile(boot_diffs, 97.5), color='blue', linewidth=2, linestyle=':')\n",
    "plt.xlabel('Amphetamine âˆ’ Control (squares entered)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Bootstrapped Distribution: Amphetamine Effect')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reproducibility-header",
   "metadata": {},
   "source": [
    "### Advantage 4: Reproducibility\n",
    "\n",
    "If someone asks *\"How did you get that result?\"*, you can hand them this notebook. Every step is documented, every analysis is re-runnable.\n",
    "\n",
    "In JASP, you'd have to write out: *\"I clicked ANOVA, then dragged Squares entered into the dependent variable box, then I dragged Group into the grouping variable, then I checked the Tukey option under Post Hoc...\"*\n",
    "\n",
    "In Python, the code **is** the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "error-box",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Errors and What They Mean\n",
    "\n",
    "When you see an error, don't panic! Here's what common errors mean:\n",
    "\n",
    "| Error | What it means | How to fix |\n",
    "|-------|---------------|------------|\n",
    "| `FileNotFoundError` | Python can't find the file | Check filename spelling, make sure file is in the same folder as the notebook |\n",
    "| `KeyError: 'column_name'` | That column doesn't exist in your data | Check spelling with `df.columns` to see all column names |\n",
    "| `ValueError: could not convert string to float` | There's text in a column that should be numbers | Check for header rows or non-numeric data with `df.head()` |\n",
    "| `NameError: name 'x' is not defined` | You're using a variable before creating it | Make sure you ran the cell that creates that variable first |\n",
    "| `IndentationError` | Python code isn't lined up correctly | Check that spaces/tabs are consistent |\n",
    "\n",
    "**Pro tip:** Read error messages from the bottom up â€” the last line usually tells you what went wrong!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rm-anova-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus: Repeated Measures ANOVA (Optional)\n",
    "\n",
    "Repeated measures designs (same subjects measured multiple times) require special handling. Python can do this using the `pingouin` library, but it requires additional setup.\n",
    "\n",
    "**This section is optional and won't be covered in today's lab.**\n",
    "\n",
    "If you want to explore on your own:\n",
    "1. Install pingouin: `pip install pingouin`\n",
    "2. See the documentation: [pingouin.rm_anova](https://pingouin-stats.org/generated/pingouin.rm_anova.html)\n",
    "\n",
    "Example code (don't run unless you have pingouin installed):\n",
    "\n",
    "```python\n",
    "import pingouin as pg\n",
    "\n",
    "# Repeated measures ANOVA\n",
    "pg.rm_anova(data=df, dv='Score', within='Time', subject='Subject')\n",
    "\n",
    "# Mixed ANOVA (within + between factors)\n",
    "pg.mixed_anova(data=df, dv='Score', within='Time', between='Group', subject='Subject')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recap",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint: JASP vs Python\n",
    "\n",
    "You just replicated **every test** from the JASP lab in Python. Let's count the lines of code:\n",
    "\n",
    "| Test | Python Code | JASP |\n",
    "|------|-----------|------|\n",
    "| Independent t-test | `stats.ttest_ind(a, b)` â€” **1 line** | 4-5 clicks, drag variables |\n",
    "| Paired t-test | `stats.ttest_rel(a, b)` â€” **1 line** | Reshape data, 4-5 clicks |\n",
    "| Welch's t-test | `stats.ttest_ind(a, b, equal_var=False)` â€” **1 line** | Buried in options menu |\n",
    "| Levene's test | `stats.levene(a, b)` â€” **1 line** | Separate analysis |\n",
    "| One-way ANOVA | `stats.f_oneway(g1, g2, ...)` â€” **1 line** | 3-4 clicks, check boxes |\n",
    "| Tukey HSD | `tukey_hsd(g1, g2, ...)` â€” **1 line** | Check post-hoc options |\n",
    "| 28 pairwise tests | Loop: 8 lines total | Click 28Ã— through menus |\n",
    "\n",
    "The tests are equally simple. **But Python can do things JASP cannot.** We demonstrated:\n",
    "- Automation (loop through all comparisons)\n",
    "- Custom publication-quality visualizations\n",
    "- Bootstrap confidence intervals\n",
    "- Complete reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: The Python Cheat Sheet\n",
    "\n",
    "| What you want to do | Python code |\n",
    "|---|---|\n",
    "| Load Excel data | `df = pd.read_excel('file.xlsx')` |\n",
    "| Load CSV data | `df = pd.read_csv('file.csv')` |\n",
    "| View first rows | `df.head()` |\n",
    "| Check data structure | `df.info()` |\n",
    "| Check for missing values | `df.isna().sum()` |\n",
    "| Group means | `df.groupby('Group')['DV'].mean()` |\n",
    "| Descriptive stats | `df.groupby('Group')['DV'].agg(['count', 'mean', 'std'])` |\n",
    "| Independent t-test | `stats.ttest_ind(group1, group2)` |\n",
    "| Welch's t-test | `stats.ttest_ind(group1, group2, equal_var=False)` |\n",
    "| Paired t-test | `stats.ttest_rel(pre, post)` |\n",
    "| Levene's test | `stats.levene(group1, group2)` |\n",
    "| One-way ANOVA | `stats.f_oneway(g1, g2, g3, ...)` |\n",
    "| Tukey HSD | `tukey_hsd(g1, g2, g3, ...)` |\n",
    "| Bar plot | `plt.bar(x, heights, yerr=errors)` |\n",
    "| Histogram | `plt.hist(x, bins=30)` |\n",
    "| Boxplot | `plt.boxplot([g1, g2], labels=['A', 'B'])` |\n",
    "\n",
    "**That's it.** With these functions, you can do everything JASP does â€” and much more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
