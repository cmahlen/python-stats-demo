{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac1b4b18ecf",
   "metadata": {},
   "source": "# fMRI Lab: Day 2 Validation\n\n## EXPLORATORY ANALYSIS\n\nWelcome back. Today you will find out whether the findings you presented in class hold up in an independent dataset."
  },
  {
   "cell_type": "markdown",
   "id": "28af6770a64",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Set Your Topic and Reload Data\n",
    "\n",
    "Enter your topic below (the same one from Day 1), then run all the cells in order. Your Colab runtime is fresh, so you need to reinstall packages and re-download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dc7a384f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET YOUR TOPIC (must match Day 1)\n",
    "# Options: 'pain', 'depression', or 'anxiety'\n",
    "\n",
    "TOPIC = 'depression'   # <-- change this to match your Day 1 assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ca579b1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages and download data files\n",
    "import subprocess, sys\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'nilearn', 'statsmodels', '-q'])\n",
    "\n",
    "import os, urllib.request\n",
    "base_url = 'https://raw.githubusercontent.com/cmahlen/python-stats-demo/main/'\n",
    "files_needed = [\n",
    "    'lab_helpers.py', 'atlas_labels.txt', 'data/roi_mni_coords.npy',\n",
    "    f'data/{TOPIC}_discovery.npz', f'data/{TOPIC}_validation.npz',\n",
    "]\n",
    "os.makedirs('data', exist_ok=True)\n",
    "for f in files_needed:\n",
    "    if not os.path.exists(f):\n",
    "        urllib.request.urlretrieve(base_url + f, f)\n",
    "\n",
    "import lab_helpers as helpers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec37d91dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run discovery analysis to reconstruct your Day 1 results\n",
    "helpers.load_dataset(TOPIC, 'discovery')\n",
    "results = helpers.test_all_edges()\n",
    "\n",
    "n_sig = (results['p'] < 0.05).sum()\n",
    "print(f\"Discovery dataset: {n_sig:,} edges significant at p < 0.05 (uncorrected)\")\n",
    "print(f\"\\nTop 5 findings from discovery:\")\n",
    "print(results.head(5)[['ROI_A', 'ROI_B', 'r', 'p']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7486967375",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Commit Your Claimed Findings\n",
    "\n",
    "**Before we load the validation data**, enter the specific edges you are planning to present. Copy the exact ROI names from your Day 1 notebook output.\n",
    "\n",
    "This step matters: you are committing to your claims before seeing whether they replicate. That is what makes this a real test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345e97d94aa",
   "metadata": {},
   "outputs": [],
   "source": "# Enter the edges you are claiming as your findings.\n# Copy the exact ROI names from your Day 1 results table.\n# Format: ('ROI_A_name_here', 'ROI_B_name_here')\n\nmy_findings = [\n    ('LH_DefaultB_PFCd_3', 'LH_TempPar_2'),   # <-- Replace with your actual edge(s)\n    # Add more rows if you are claiming multiple findings:\n    # ('ROI_A_name_here', 'ROI_B_name_here'),\n]\n\n# Enter the same analytic choices you used in Day 1:\ncovariates        = None   # e.g. ['Physical_Activity', 'Screen_Time'] or None\noutlier_threshold = None   # e.g. 2 or None\nsubgroup          = None   # e.g. {'Sex': 0} or None\n\nprint(\"Your claimed findings:\")\nfor i, (a, b) in enumerate(my_findings, 1):\n    print(f\"  {i}. {a} <-> {b}\")\nprint(f\"\\nCovariates: {covariates}  |  Outlier threshold: {outlier_threshold}  |  Subgroup: {subgroup}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f439cbf5-4396-47d8-aabd-8779afb0d709",
   "metadata": {},
   "outputs": [],
   "source": "outcome_col = helpers.get_behavior().columns[0]\n\nfor roi_a, roi_b in my_findings:\n    helpers.plot_edge(roi_a, roi_b, behavior_col=outcome_col,\n                      covariates=covariates,\n                      exclude_outliers=outlier_threshold,\n                      subgroup=subgroup)"
  },
  {
   "cell_type": "markdown",
   "id": "d9aebbdf9e0",
   "metadata": {},
   "source": "---\n\n## Part 3: Validate Your Claimed Findings\n\nNow load the validation dataset and test whether your claimed edges replicate. A finding replicates if it is statistically significant in the same direction in both datasets.\n\nThe code below is more complex than what you wrote on Day 1. You do not need to understand every line; just run the cell and focus on the output."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457340ebe12",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 80)\nprint(\"VALIDATION OF YOUR CLAIMED FINDINGS\")\nprint(\"=\" * 80)\n\nclaimed_results = []\nfor roi_a, roi_b in my_findings:\n    # Discovery (using the same analytic choices you entered above)\n    helpers.load_dataset(TOPIC, 'discovery')\n    r_disc, p_disc, n_disc = helpers.test_edge(\n        roi_a, roi_b, behavior_col=outcome_col,\n        covariates=covariates, exclude_outliers=outlier_threshold, subgroup=subgroup\n    )\n\n    # Validation (same analytic choices applied to new data)\n    helpers.load_dataset(TOPIC, 'validation')\n    r_val, p_val, n_val = helpers.test_edge(\n        roi_a, roi_b, behavior_col=outcome_col,\n        covariates=covariates, exclude_outliers=outlier_threshold, subgroup=subgroup\n    )\n\n    same_direction = (r_val * r_disc) > 0\n    if p_val < 0.05 and same_direction:\n        replicated = 'YES'\n    elif p_val < 0.05 and not same_direction:\n        replicated = 'FLIPPED'\n    else:\n        replicated = 'NO'\n\n    claimed_results.append({\n        'ROI_A': roi_a,\n        'ROI_B': roi_b,\n        'Discovery_r': round(r_disc, 3),\n        'Discovery_p': round(p_disc, 4),\n        'Validation_r': round(r_val, 3),\n        'Validation_p': round(p_val, 4),\n        'Replicated': replicated,\n    })\n\nclaimed_df = pd.DataFrame(claimed_results)\nclaimed_df.index = range(1, len(claimed_df) + 1)\nprint(claimed_df.to_string())\n\nn_rep = (claimed_df['Replicated'] == 'YES').sum()\nprint(f\"\\n{n_rep} of {len(my_findings)} claimed finding(s) replicated.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5143c0a3e",
   "metadata": {},
   "outputs": [],
   "source": "# Scatter plots for your claimed findings (discovery then validation)\n# Analytic choices from above are applied to both\nfor roi_a, roi_b in my_findings:\n    print(f\"\\n{'='*60}\")\n    print(f\"{roi_a} <-> {roi_b}\")\n    print(f\"{'='*60}\")\n\n    print(\"Discovery set:\")\n    helpers.load_dataset(TOPIC, 'discovery')\n    helpers.plot_edge(roi_a, roi_b, behavior_col=outcome_col,\n                      covariates=covariates,\n                      exclude_outliers=outlier_threshold,\n                      subgroup=subgroup)\n\n    print(\"Validation set:\")\n    helpers.load_dataset(TOPIC, 'validation')\n    helpers.plot_edge(roi_a, roi_b, behavior_col=outcome_col,\n                      covariates=covariates,\n                      exclude_outliers=outlier_threshold,\n                      subgroup=subgroup)"
  },
  {
   "cell_type": "markdown",
   "id": "02d724e92c6",
   "metadata": {},
   "source": "---\n\n## Part 4: Validate Your Top 5 Discoveries\n\nNow let's test the 5 statistically strongest edges from your discovery dataset, regardless of which ones you chose to present. This is an objective baseline.\n\nAfter you see the results, compare: how many of your claimed findings overlap with the top 5? Which set replicated better, and why?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd4d8dd3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5 = results.head(5)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"VALIDATION OF TOP 5 DISCOVERY EDGES (by p-value)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "top5_results = []\n",
    "for _, row in top_5.iterrows():\n",
    "    r_disc = row['r']\n",
    "    p_disc = row['p']\n",
    "\n",
    "    helpers.load_dataset(TOPIC, 'validation')\n",
    "    val_edge = helpers.get_edge(row['ROI_A'], row['ROI_B'])\n",
    "    val_out = helpers.get_behavior()[outcome_col].values\n",
    "    r_val, p_val = pearsonr(val_edge, val_out)\n",
    "\n",
    "    same_direction = (r_val * r_disc) > 0\n",
    "    if p_val < 0.05 and same_direction:\n",
    "        replicated = 'YES'\n",
    "    elif p_val < 0.05 and not same_direction:\n",
    "        replicated = 'FLIPPED'\n",
    "    else:\n",
    "        replicated = 'NO'\n",
    "\n",
    "    top5_results.append({\n",
    "        'ROI_A': row['ROI_A'],\n",
    "        'ROI_B': row['ROI_B'],\n",
    "        'Discovery_r': round(r_disc, 3),\n",
    "        'Discovery_p': round(p_disc, 4),\n",
    "        'Validation_r': round(r_val, 3),\n",
    "        'Validation_p': round(p_val, 4),\n",
    "        'Replicated': replicated,\n",
    "    })\n",
    "\n",
    "top5_df = pd.DataFrame(top5_results)\n",
    "top5_df.index = range(1, len(top5_df) + 1)\n",
    "print(top5_df.to_string())\n",
    "\n",
    "n_rep = (top5_df['Replicated'] == 'YES').sum()\n",
    "print(f\"\\n{n_rep} of 5 top discoveries replicated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a7c7d5690",
   "metadata": {},
   "source": [
    "How many of your claimed findings (Part 3) overlap with the top 5? Which set replicated better, and why might that be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tofob2vbo19",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Reflection Questions\n",
    "\n",
    "Before we discuss as a group, take a moment to think about these questions:\n",
    "\n",
    "1. **How confident were you that your FDR-corrected discoveries were real?**\n",
    "   - What would have made you more or less confident?\n",
    "\n",
    "2. **The multiple comparisons problem**\n",
    "   - You narrowed from 23,220 edges to a specific network to get a gentler FDR correction. Is that the whole story?\n",
    "   - How many different networks, covariates, and subgroups did you try before finding something that survived correction?\n",
    "   - Does the FDR correction account for ALL the tests you actually ran?\n",
    "\n",
    "3. **Analytic flexibility**\n",
    "   - How many different analysis configurations did you try?\n",
    "   - If you tried multiple approaches, which results did you choose to present and why?\n",
    "\n",
    "4. **The extra credit incentive**\n",
    "   - Did the extra credit incentive affect your analysis decisions?\n",
    "   - Did you feel pressure to find results that survived FDR correction?\n",
    "   - How is this similar to pressures in real academic research?\n",
    "\n",
    "5. **Effect sizes**\n",
    "   - How strong were the correlations you found (r values)?\n",
    "   - Are these large enough to be practically meaningful?\n",
    "\n",
    "Let's discuss these briefly before the class wrap-up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0524833",
   "metadata": {},
   "source": "---\n\n## What Did We Learn?\n\nIf your top findings **did not replicate**, you are in good company. This is the norm for underpowered, exploratory analyses. The findings were real in the discovery dataset, but they were likely false positives driven by chance.\n\nThis is not a failure. It is the lesson. The reproducibility crisis in science is largely caused by exactly what you just did: testing many things, finding something that works, and reporting it as if it were the only thing you tested.\n\n**The hypothesis-driven group** pre-registered their analysis before running it. We will find out whether their approach fared better and why.\n\nIf your findings **did replicate**: think carefully about whether you ran a truly hypothesis-driven analysis, or whether you found your \"hypothesis\" after exploring the data."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}