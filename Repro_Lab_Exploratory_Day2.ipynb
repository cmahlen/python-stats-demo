{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac1b4b18ecf",
   "metadata": {},
   "source": [
    "# Reproducibility Lab: Day 2 -- Validation\n",
    "\n",
    "## EXPLORATORY ANALYSIS\n",
    "\n",
    "Welcome back. Today you will find out whether the findings you presented in class hold up in an independent dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28af6770a64",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Set Your Topic and Reload Data\n",
    "\n",
    "Enter your topic below (the same one from Day 1), then run all the cells in order. Your Colab runtime is fresh, so you need to reinstall packages and re-download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dc7a384f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET YOUR TOPIC (must match Day 1)\n",
    "# Options: 'pain', 'depression', or 'anxiety'\n",
    "\n",
    "TOPIC = 'depression'   # <-- change this to match your Day 1 assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ca579b1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages and download data files\n",
    "import subprocess, sys\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'nilearn', 'statsmodels', '-q'])\n",
    "\n",
    "import os, urllib.request\n",
    "base_url = 'https://raw.githubusercontent.com/cmahlen/python-stats-demo/main/'\n",
    "files_needed = [\n",
    "    'lab_helpers.py', 'atlas_labels.txt', 'data/roi_mni_coords.npy',\n",
    "    f'data/{TOPIC}_discovery.npz', f'data/{TOPIC}_validation.npz',\n",
    "]\n",
    "os.makedirs('data', exist_ok=True)\n",
    "for f in files_needed:\n",
    "    if not os.path.exists(f):\n",
    "        urllib.request.urlretrieve(base_url + f, f)\n",
    "\n",
    "import lab_helpers as helpers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec37d91dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run discovery analysis to reconstruct your Day 1 results\n",
    "helpers.load_dataset(TOPIC, 'discovery')\n",
    "results = helpers.test_all_edges()\n",
    "\n",
    "n_sig = (results['p'] < 0.05).sum()\n",
    "print(f\"Discovery dataset: {n_sig:,} edges significant at p < 0.05 (uncorrected)\")\n",
    "print(f\"\\nTop 5 findings from discovery:\")\n",
    "print(results.head(5)[['ROI_A', 'ROI_B', 'r', 'p']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7486967375",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Commit Your Claimed Findings\n",
    "\n",
    "**Before we load the validation data**, enter the specific edges you are planning to present. Copy the exact ROI names from your Day 1 notebook output.\n",
    "\n",
    "This step matters: you are committing to your claims before seeing whether they replicate. That is what makes this a real test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345e97d94aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the edges you are claiming as your findings.\n",
    "# Copy the exact ROI names from your Day 1 results table.\n",
    "# Format: ('ROI_A_name_here', 'ROI_B_name_here')\n",
    "\n",
    "my_findings = [\n",
    "    ('LH_DefaultB_PFCd_3', 'LH_TempPar_2'),   # <-- Replace with your actual edge(s)\n",
    "    # Add more rows if you are claiming multiple findings:\n",
    "    # ('ROI_A_name_here', 'ROI_B_name_here'),\n",
    "]\n",
    "\n",
    "print(\"Your claimed findings:\")\n",
    "for i, (a, b) in enumerate(my_findings, 1):\n",
    "    print(f\"  {i}. {a} <-> {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f439cbf5-4396-47d8-aabd-8779afb0d709",
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.plot_edge(\"LH_TempPar_2\", \"LH_DefaultB_PFCd_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9aebbdf9e0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Validate Your Claimed Findings\n",
    "\n",
    "Now load the validation dataset and test whether your claimed edges replicate. A finding replicates if it is statistically significant in the same direction in both datasets.\n",
    "\n",
    "The code below is more complex than what you wrote on Day 1. You do not need to understand every line -- run the cell and focus on the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457340ebe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_col = helpers.get_behavior().columns[0]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"VALIDATION OF YOUR CLAIMED FINDINGS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "claimed_results = []\n",
    "for roi_a, roi_b in my_findings:\n",
    "    # Discovery\n",
    "    helpers.load_dataset(TOPIC, 'discovery')\n",
    "    disc_edge = helpers.get_edge(roi_a, roi_b)\n",
    "    disc_out = helpers.get_behavior()[outcome_col].values\n",
    "    r_disc, p_disc = pearsonr(disc_edge, disc_out)\n",
    "\n",
    "    # Validation\n",
    "    helpers.load_dataset(TOPIC, 'validation')\n",
    "    val_edge = helpers.get_edge(roi_a, roi_b)\n",
    "    val_out = helpers.get_behavior()[outcome_col].values\n",
    "    r_val, p_val = pearsonr(val_edge, val_out)\n",
    "\n",
    "    same_direction = (r_val * r_disc) > 0\n",
    "    if p_val < 0.05 and same_direction:\n",
    "        replicated = 'YES'\n",
    "    elif p_val < 0.05 and not same_direction:\n",
    "        replicated = 'FLIPPED'\n",
    "    else:\n",
    "        replicated = 'NO'\n",
    "\n",
    "    claimed_results.append({\n",
    "        'ROI_A': roi_a,\n",
    "        'ROI_B': roi_b,\n",
    "        'Discovery_r': round(r_disc, 3),\n",
    "        'Discovery_p': round(p_disc, 4),\n",
    "        'Validation_r': round(r_val, 3),\n",
    "        'Validation_p': round(p_val, 4),\n",
    "        'Replicated': replicated,\n",
    "    })\n",
    "\n",
    "claimed_df = pd.DataFrame(claimed_results)\n",
    "claimed_df.index = range(1, len(claimed_df) + 1)\n",
    "print(claimed_df.to_string())\n",
    "\n",
    "n_rep = (claimed_df['Replicated'] == 'YES').sum()\n",
    "print(f\"\\n{n_rep} of {len(my_findings)} claimed finding(s) replicated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5143c0a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side scatter plots for your claimed findings\n",
    "for roi_a, roi_b in my_findings:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "    # Discovery\n",
    "    helpers.load_dataset(TOPIC, 'discovery')\n",
    "    disc_edge = helpers.get_edge(roi_a, roi_b)\n",
    "    disc_out = helpers.get_behavior()[outcome_col].values\n",
    "    r_d, p_d = pearsonr(disc_edge, disc_out)\n",
    "\n",
    "    axes[0].scatter(disc_edge, disc_out, alpha=0.5, color='steelblue')\n",
    "    z = np.polyfit(disc_edge, disc_out, 1)\n",
    "    x_line = np.linspace(disc_edge.min(), disc_edge.max(), 100)\n",
    "    axes[0].plot(x_line, np.polyval(z, x_line), color='coral', linewidth=2)\n",
    "    axes[0].set_xlabel('Functional Connectivity')\n",
    "    axes[0].set_ylabel(outcome_col)\n",
    "    axes[0].set_title(f'Discovery Set\\nr = {r_d:.3f}, p = {p_d:.2e}')\n",
    "\n",
    "    # Validation\n",
    "    helpers.load_dataset(TOPIC, 'validation')\n",
    "    val_edge = helpers.get_edge(roi_a, roi_b)\n",
    "    val_out = helpers.get_behavior()[outcome_col].values\n",
    "    r_v, p_v = pearsonr(val_edge, val_out)\n",
    "\n",
    "    axes[1].scatter(val_edge, val_out, alpha=0.5, color='mediumseagreen')\n",
    "    z = np.polyfit(val_edge, val_out, 1)\n",
    "    x_line = np.linspace(val_edge.min(), val_edge.max(), 100)\n",
    "    axes[1].plot(x_line, np.polyval(z, x_line), color='coral', linewidth=2)\n",
    "    axes[1].set_xlabel('Functional Connectivity')\n",
    "    axes[1].set_ylabel(outcome_col)\n",
    "    axes[1].set_title(f'Validation Set\\nr = {r_v:.3f}, p = {p_v:.2e}')\n",
    "\n",
    "    fig.suptitle(f'{roi_a} <-> {roi_b}', fontsize=13)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d724e92c6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Validate Your Top 5 Discoveries\n",
    "\n",
    "Now let's test the 5 statistically strongest edges from your discovery dataset -- regardless of which ones you chose to present. This is an objective baseline.\n",
    "\n",
    "After you see the results, compare: how many of your claimed findings overlap with the top 5? Which set replicated better, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd4d8dd3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5 = results.head(5)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"VALIDATION OF TOP 5 DISCOVERY EDGES (by p-value)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "top5_results = []\n",
    "for _, row in top_5.iterrows():\n",
    "    r_disc = row['r']\n",
    "    p_disc = row['p']\n",
    "\n",
    "    helpers.load_dataset(TOPIC, 'validation')\n",
    "    val_edge = helpers.get_edge(row['ROI_A'], row['ROI_B'])\n",
    "    val_out = helpers.get_behavior()[outcome_col].values\n",
    "    r_val, p_val = pearsonr(val_edge, val_out)\n",
    "\n",
    "    same_direction = (r_val * r_disc) > 0\n",
    "    if p_val < 0.05 and same_direction:\n",
    "        replicated = 'YES'\n",
    "    elif p_val < 0.05 and not same_direction:\n",
    "        replicated = 'FLIPPED'\n",
    "    else:\n",
    "        replicated = 'NO'\n",
    "\n",
    "    top5_results.append({\n",
    "        'ROI_A': row['ROI_A'],\n",
    "        'ROI_B': row['ROI_B'],\n",
    "        'Discovery_r': round(r_disc, 3),\n",
    "        'Discovery_p': round(p_disc, 4),\n",
    "        'Validation_r': round(r_val, 3),\n",
    "        'Validation_p': round(p_val, 4),\n",
    "        'Replicated': replicated,\n",
    "    })\n",
    "\n",
    "top5_df = pd.DataFrame(top5_results)\n",
    "top5_df.index = range(1, len(top5_df) + 1)\n",
    "print(top5_df.to_string())\n",
    "\n",
    "n_rep = (top5_df['Replicated'] == 'YES').sum()\n",
    "print(f\"\\n{n_rep} of 5 top discoveries replicated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a7c7d5690",
   "metadata": {},
   "source": [
    "How many of your claimed findings (Part 3) overlap with the top 5? Which set replicated better, and why might that be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tofob2vbo19",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Reflection Questions\n",
    "\n",
    "Before we discuss as a group, take a moment to think about these questions:\n",
    "\n",
    "1. **How confident were you that your FDR-corrected discoveries were real?**\n",
    "   - What would have made you more or less confident?\n",
    "\n",
    "2. **The multiple comparisons problem**\n",
    "   - You narrowed from 23,220 edges to a specific network to get a gentler FDR correction. Is that the whole story?\n",
    "   - How many different networks, covariates, and subgroups did you try before finding something that survived correction?\n",
    "   - Does the FDR correction account for ALL the tests you actually ran?\n",
    "\n",
    "3. **Analytic flexibility**\n",
    "   - How many different analysis configurations did you try?\n",
    "   - If you tried multiple approaches, which results did you choose to present and why?\n",
    "\n",
    "4. **The extra credit incentive**\n",
    "   - Did the extra credit incentive affect your analysis decisions?\n",
    "   - Did you feel pressure to find results that survived FDR correction?\n",
    "   - How is this similar to pressures in real academic research?\n",
    "\n",
    "5. **Effect sizes**\n",
    "   - How strong were the correlations you found (r values)?\n",
    "   - Are these large enough to be practically meaningful?\n",
    "\n",
    "Let's discuss these briefly before the class wrap-up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0524833",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What Did We Learn?\n",
    "\n",
    "If your top findings **did not replicate**, you are in good company -- this is the norm for underpowered, uncorrected exploratory analyses. The findings were real in the discovery dataset, but they were likely false positives driven by chance.\n",
    "\n",
    "This is not a failure. It is the lesson. The reproducibility crisis in science is largely caused by exactly what you just did: testing many things, finding something that works, and reporting it as if it were the only thing you tested.\n",
    "\n",
    "**The hypothesis-driven group** pre-registered their analysis before running it. We'll find out whether their approach fared better -- and why.\n",
    "\n",
    "If your findings **did replicate**: think carefully about whether you ran a truly hypothesis-driven analysis, or whether you found your \"hypothesis\" after exploring the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}